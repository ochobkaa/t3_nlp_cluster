# Кластеризация эмбеддингов и классификация новых эмбеддингов по полученным кластерам
Что оно делает?
1. Загружает BERT эмбеддинги и снижает их размерность с 768 так, чтобы explained variance был не менее 0.95.
Мой датасет сжимается до размерности 80. Для уменьшения размерности используется IncrementalPCA, который позволяет сжимать исходный датасет по батчам, не загружая его в память целиком.
Батчи загружаются отдельными файлами в папки /embeddings_clu (датасет для кластеризации), /embeddings_classf (датасет для классификации по полученным ранее кластерам).
Эмбеддинги уменьшенной размерности нормализуются до нулевого среднего и единичного стандартного отклонения.
2. Кластеризует датасет из /embeddings_clu методом **Agglomerative Clustering** с оптимизацией количества кластеров путем минимизации **Davies-Bouldin score** и **косинусным расстоянием** в качестве метрики расстояний.
В задании было указано использовать **DBSCAN** или **K-Means** в качестве метода кластеризации, а в качестве метрики **метод локтя** или **Silhouette Score**, однако все они плохо работают с моей выборкой.
Расположение полученных мною эмбеддингов в пространстве близко к гауссовой сфере и в их расположении нет ярко разделимых структур, что отражается в визуализации t-SNE.
Поэтому при попытке применения **DBSCAN** не получилось подобрать такой эпсилон, при котором бы было несколько равномерных кластеров, все время образовывался один гигантский кластер с кучей мелких кластеров-выбросов.
При применении **К-Means** не получалось стабильного количества кластеров, поскольку между ними нет ярко выраженного разделения и потому из-за рандомизированной инициализации он не давал стабильного решения.
Метрики **метод локтя** или **Silhouette Score** также подходят только для ярко разделимых между собой кластеров, поэтому плохо подходят для моей выборки.
И поэтому выбрал метод и метрику такие, при которых получались равномерные и интерпретируемые кластеры (сочетающиеся с тематикой текстов из датасета).
3. Вычисляет **центроиды** полученных кластеров и пребразует 80 мерные вектора в вектора косинусных расстояний до каждой центроиды. 
На преобразованных векторах, которые использовались для кластеризации, обучается **мультиноминальная логистическая регрессия**.
Поскольку между кластерами нет ярко выраженных границ, они не идеально сферические и близко расположены друг к другу, логистическая регрессия как бы аппроксимирует функцию плотности каждого кластера по осям в пространстве
расстояний до центроид и таким образом помогает дифференцировать пограничные точки, равноудаленные от центроид нескольких кластеров.
4. Эмбеддинги для классикации, предварительно сжатые в пространство ранее обученного IncrementalPCA и пребразованные в вектора расстояний до центроид классифицируется по полученной логистической регрессии.
5. Визуализирует гистограмму распределения по кластерам и t-SNE диаграмму распределения точек кластеризованного датасета.
